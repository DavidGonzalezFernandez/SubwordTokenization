{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for training (generating the vocabulary) and tokenizing a text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from tokenization_methods.tokenization_method import TokenizationMethod\n",
    "from tokenization_methods.byte_pair_encoding import BytePairEncoding\n",
    "from tokenization_methods.unigram_language_model import UnigramLanguageModel\n",
    "from tokenization_methods.wordpiece import WordPiece\n",
    "\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate clean text\n",
    "def clean_text(raw_text):\n",
    "    # Remove \\n\n",
    "    raw_text = raw_text.replace(\"\\n\", \" \")\n",
    "\n",
    "    # Remove unnecessary characters\n",
    "    characters_to_remove = (\"»\", \":\", \"'\", '\"', \";\", \",\", \")\", \"(\", \"[\", \"]\", \"?\", \"¿\", \"!\", \"¡\", \"-\", \"«\", \"\\t\")\n",
    "    for c in characters_to_remove:\n",
    "        raw_text = raw_text.replace(c, \" \")\n",
    "\n",
    "    # Remove accents\n",
    "    raw_text = raw_text.lower()\n",
    "    raw_text = raw_text.replace(\"á\", \"a\")\n",
    "    raw_text = raw_text.replace(\"ä\", \"a\")\n",
    "    raw_text = raw_text.replace(\"à\", \"a\")\n",
    "    raw_text = raw_text.replace(\"é\", \"e\")\n",
    "    raw_text = raw_text.replace(\"ë\", \"e\")\n",
    "    raw_text = raw_text.replace(\"è\", \"e\")\n",
    "    raw_text = raw_text.replace(\"í\", \"i\")\n",
    "    raw_text = raw_text.replace(\"ï\", \"i\")\n",
    "    raw_text = raw_text.replace(\"ì\", \"i\")\n",
    "    raw_text = raw_text.replace(\"ó\", \"o\")\n",
    "    raw_text = raw_text.replace(\"ö\", \"o\")\n",
    "    raw_text = raw_text.replace(\"ò\", \"o\")\n",
    "    raw_text = raw_text.replace(\"ú\", \"u\")\n",
    "    raw_text = raw_text.replace(\"ü\", \"u\")\n",
    "    raw_text = raw_text.replace(\"ù\", \"u\")\n",
    "\n",
    "    # Remove numbers\n",
    "    numbers = [str(i) for i in range(0, 10)]\n",
    "    for c in numbers:\n",
    "        raw_text = raw_text.replace(c, \" \")\n",
    "\n",
    "    # Remove double spaces\n",
    "    while \"  \" in raw_text:\n",
    "        raw_text = raw_text.replace(\"  \", \" \")\n",
    "\n",
    "    return raw_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all the lines (Training corpus is Don Quijote)\n",
    "with open(\"quijote.txt\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# All the lines in the same string\n",
    "raw_text = \" \".join(lines)\n",
    "\n",
    "# Clean text\n",
    "sentences = clean_text(raw_text).split(\".\")\n",
    "\n",
    "# Make the corpus be made up from each of the sentences in the text.\n",
    "corpus = [sentence.strip() for sentence in sentences]\n",
    "corpus = [sentence for sentence in corpus if sentence]\n",
    "corpus = [sentence for sentence in corpus if not sentence.startswith(\"capitulo\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_FOLDER = \"results\"\n",
    "if not os.path.exists(RESULTS_FOLDER):\n",
    "    os.mkdir(RESULTS_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZES = {\n",
    "    \"tiny\": 250,\n",
    "    \"small\": 1_000,\n",
    "    \"medium\": 4_000,\n",
    "    \"large\": 16_000\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bpe_vocab(vocab_size, file_name):\n",
    "    if not os.path.exists(file_name):\n",
    "        vocab = BytePairEncoding().create_vocabulary(corpus=corpus, vocab_size=vocab_size)\n",
    "        with open(file_name, \"wb\") as f:\n",
    "            pickle.dump(vocab, f)\n",
    "\n",
    "    with open(file_name, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def get_bpe_tiny_vocab():\n",
    "    return get_bpe_vocab(VOCAB_SIZES[\"tiny\"], os.path.join(RESULTS_FOLDER, \"vocab_bpe_tiny.pkl\"))\n",
    "\n",
    "def get_bpe_small_vocab():\n",
    "    return get_bpe_vocab(VOCAB_SIZES[\"small\"], os.path.join(RESULTS_FOLDER, \"vocab_bpe_small.pkl\"))\n",
    "\n",
    "def get_bpe_medium_vocab():\n",
    "    return get_bpe_vocab(VOCAB_SIZES[\"medium\"], os.path.join(RESULTS_FOLDER, \"vocab_bpe_medium.pkl\"))\n",
    "\n",
    "def get_bpe_large_vocab():\n",
    "    return get_bpe_vocab(VOCAB_SIZES[\"large\"], os.path.join(RESULTS_FOLDER, \"vocab_bpe_large.pkl\"))\n",
    "\n",
    "def get_bpe_vocabs():\n",
    "    tiny = get_bpe_tiny_vocab()\n",
    "    small = get_bpe_small_vocab()\n",
    "    medium = get_bpe_medium_vocab()\n",
    "    large = get_bpe_large_vocab()\n",
    "\n",
    "    return tiny,small,medium,large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unigram_vocab(vocab_size, file_name, initial_vocab):\n",
    "    if not os.path.exists(file_name):\n",
    "        print(\"Training file: \", file_name)\n",
    "        vocab = UnigramLanguageModel().create_vocabulary(\n",
    "            corpus=corpus[:500],        # NOTE: I don't use the full corpus\n",
    "            vocab_size=vocab_size,\n",
    "            starting_vocabulary = initial_vocab\n",
    "        )\n",
    "        with open(file_name, \"wb\") as f:\n",
    "            pickle.dump(vocab, f)\n",
    "\n",
    "    with open(file_name, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def get_unigram_tiny_vocab():\n",
    "    return get_unigram_vocab(\n",
    "        VOCAB_SIZES[\"tiny\"],\n",
    "        os.path.join(RESULTS_FOLDER, f\"vocab_unigram_tiny.pkl\"),\n",
    "        initial_vocab = get_unigram_small_vocab()\n",
    "    )\n",
    "\n",
    "def get_unigram_small_vocab():\n",
    "    return get_unigram_vocab(\n",
    "        VOCAB_SIZES[\"small\"],\n",
    "        os.path.join(RESULTS_FOLDER, f\"vocab_unigram_small.pkl\"),\n",
    "        initial_vocab = get_unigram_medium_vocab()\n",
    "    )\n",
    "\n",
    "def get_unigram_medium_vocab():\n",
    "    return get_unigram_vocab(\n",
    "        VOCAB_SIZES[\"medium\"],\n",
    "        os.path.join(RESULTS_FOLDER, f\"vocab_unigram_medium.pkl\"),\n",
    "        initial_vocab = None\n",
    "    )\n",
    "\n",
    "def get_unigram_large_vocab():\n",
    "    return get_unigram_vocab(\n",
    "        VOCAB_SIZES[\"large\"],\n",
    "        os.path.join(RESULTS_FOLDER, \"vocab_unigram_large.pkl\"),\n",
    "        initial_vocab = None\n",
    "    )\n",
    "\n",
    "def get_unigram_vocabs():\n",
    "    # large = get_unigram_large_vocab()\n",
    "    medium = get_unigram_medium_vocab()\n",
    "    small = get_unigram_small_vocab()\n",
    "    tiny = get_unigram_tiny_vocab()\n",
    "\n",
    "    return tiny,small,medium#,large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wp_vocab(vocab_size, file_name):\n",
    "    if not os.path.exists(file_name):\n",
    "        print(\"Training file: \", file_name)\n",
    "        vocab = WordPiece().create_vocabulary(corpus=corpus, vocab_size=vocab_size)\n",
    "        with open(file_name, \"wb\") as f:\n",
    "            pickle.dump(vocab, f)\n",
    "            \n",
    "    with open(file_name, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def get_wp_tiny_vocab():\n",
    "    return get_wp_vocab(VOCAB_SIZES[\"tiny\"], os.path.join(RESULTS_FOLDER, \"vocab_wp_tiny.pkl\"))\n",
    "\n",
    "def get_wp_small_vocab():\n",
    "    return get_wp_vocab(VOCAB_SIZES[\"small\"], os.path.join(RESULTS_FOLDER, \"vocab_wp_small.pkl\"))\n",
    "\n",
    "def get_wp_medium_vocab():\n",
    "    return get_wp_vocab(VOCAB_SIZES[\"medium\"], os.path.join(RESULTS_FOLDER, \"vocab_wp_medium.pkl\"))\n",
    "\n",
    "def get_wp_large_vocab():\n",
    "    return get_wp_vocab(VOCAB_SIZES[\"large\"], os.path.join(RESULTS_FOLDER, \"vocab_wp_large.pkl\"))\n",
    "\n",
    "def get_wp_vocabs():\n",
    "    tiny = get_wp_tiny_vocab()\n",
    "    small = get_wp_small_vocab()\n",
    "    medium = get_wp_medium_vocab()\n",
    "    large = get_wp_large_vocab()\n",
    "\n",
    "    return tiny,small,medium,large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpe_vocabs = get_bpe_vocabs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for vocab_smaller, vocab_larger in zip(bpe_vocabs, bpe_vocabs[1:]):\n",
    "    assert vocab_larger[:len(vocab_smaller)] == vocab_smaller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_vocabs = get_unigram_vocabs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for vocab_smaller, vocab_larger in zip(unigram_vocabs, unigram_vocabs[1:]):\n",
    "    assert all(tk in vocab_larger for tk in vocab_smaller) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "wp_vocabs = get_wp_vocabs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for vocab_smaller, vocab_larger in zip(wp_vocabs, wp_vocabs[1:]):\n",
    "    assert vocab_larger[:len(vocab_smaller)] == vocab_smaller"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "asimov_law_0 = \"Un robot no puede dañar a la humanidad o, por inacción, permitir que la humanidad sufra daños.\"\n",
    "asimov_law_1 = \"Un robot no hará daño a un ser humano, ni por inacción permitirá que un ser humano sufra daño.\"\n",
    "asimov_law_2 = \"Un robot debe cumplir las órdenes dadas por los seres humanos, a excepción de aquellas que entren en conflicto con la primera ley.\"\n",
    "asimov_law_3 = \"Un robot debe proteger su propia existencia en la medida en que esta protección no entre en conflicto con la primera o con la segunda ley.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_tokenize = clean_text(asimov_law_0).replace(\".\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "concrete_words_to_analyze = [\"dañar\", \"inaccion\", \"humanidad\", \"robot\"]\n",
    "assert all(w in text_to_tokenize for w in concrete_words_to_analyze)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size: 250\n",
      "Text:  [107, 137, 'b', 'o', 't', '_', 57, 96, 'e', 38, 'd', 214, 78, 154, 'h', 'u', 155, 'i', 202, 28, 86, 102, 'a', 'c', 54, 51, 198, 60, 79, 152, 34, 46, 'h', 'u', 155, 'i', 202, 64, 'f', 119, 'd', 214, 90]\n",
      "dañar ['d', 214, 42]\n",
      "inaccion [102, 'a', 'c', 54, 37]\n",
      "humanidad ['h', 'u', 155, 'i', 'd', 49]\n",
      "robot [137, 'b', 'o', 't']\n",
      "\n",
      "Size: 1000\n",
      "Text:  [107, 137, 620, 't', '_', 57, 586, 'd', 214, 78, 154, 292, 155, 'i', 202, 28, 86, 102, 638, 339, 198, 60, 79, 152, 731, 292, 155, 'i', 202, 64, 'f', 119, 'd', 214, 90]\n",
      "dañar ['d', 214, 42]\n",
      "inaccion [102, 638, 54, 37]\n",
      "humanidad [292, 155, 'i', 314]\n",
      "robot [137, 620, 't']\n",
      "\n",
      "Size: 4000\n",
      "Text:  [107, 137, 620, 't', '_', 57, 586, 2400, 78, 154, 2337, 'i', 202, 1160, 102, 638, 339, 3769, 79, 152, 731, 2337, 'i', 202, 64, 'f', 119, 2400, 90]\n",
      "dañar [2400, 42]\n",
      "inaccion [102, 638, 1770]\n",
      "humanidad [2337, 'i', 314]\n",
      "robot [137, 620, 't']\n",
      "\n",
      "Size: 16000\n",
      "Text:  [107, 137, 620, 5161, 5918, 2400, 6382, 2337, 10829, 1160, 102, 638, 339, 5881, 152, 731, 2337, 10829, 9319, 119, 2400, 90]\n",
      "dañar [2400, 42]\n",
      "inaccion [102, 7839]\n",
      "humanidad [2337, 'i', 314]\n",
      "robot [137, 620, 't']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer: TokenizationMethod = BytePairEncoding()\n",
    "\n",
    "for vocab in bpe_vocabs:\n",
    "    print(f\"Size: {len(vocab)}\")\n",
    "    print(\"Text: \", tokenizer.tokenize_text(vocab,text_to_tokenize))\n",
    "    for word_to_tokenize in concrete_words_to_analyze:\n",
    "        print(word_to_tokenize, tokenizer.tokenize_text(vocab,word_to_tokenize))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size: 250\n",
      "Text:  [('u', 'n'), ('r', 'o', 'b', 'o', 't'), ('n', 'o'), ('p', 'u', 'e', 'de'), ('d', 'a', 'ñ', 'a', 'r'), ('a',), ('l', 'a'), ('h', 'u', 'm', 'a', 'n', 'idad'), ('o',), ('por',), ('i', 'n', 'a', 'c', 'cion'), ('per', 'mi', 't', 'i', 'r'), ('que',), ('l', 'a'), ('h', 'u', 'm', 'a', 'n', 'idad'), ('s', 'u', 'f', 'r', 'a'), ('d', 'a', 'ñ', 'o', 's')]\n",
      "dañar [('d', 'a', 'ñ', 'a', 'r')]\n",
      "inaccion [('i', 'n', 'a', 'c', 'cion')]\n",
      "humanidad [('h', 'u', 'm', 'a', 'n', 'idad')]\n",
      "robot [('r', 'o', 'b', 'o', 't')]\n",
      "\n",
      "Size: 1000\n",
      "Text:  [('u', 'n'), ('r', 'o', 'b', 'o', 't'), ('n', 'o'), ('p', 'u', 'e', 'de'), ('d', 'a', 'ñ', 'a', 'r'), ('a',), ('l', 'a'), ('h', 'u', 'm', 'a', 'n', 'idad'), ('o',), ('por',), ('i', 'n', 'a', 'c', 'cion'), ('per', 'mi', 't', 'i', 'r'), ('que',), ('l', 'a'), ('h', 'u', 'm', 'a', 'n', 'idad'), ('s', 'u', 'f', 'r', 'a'), ('d', 'a', 'ñ', 'o', 's')]\n",
      "dañar [('d', 'a', 'ñ', 'a', 'r')]\n",
      "inaccion [('i', 'n', 'a', 'c', 'cion')]\n",
      "humanidad [('h', 'u', 'm', 'a', 'n', 'idad')]\n",
      "robot [('r', 'o', 'b', 'o', 't')]\n",
      "\n",
      "Size: 4000\n",
      "Text:  [('u', 'n'), ('r', 'o', 'b', 'o', 't'), ('n', 'o'), ('p', 'u', 'e', 'de'), ('d', 'a', 'ñ', 'a', 'r'), ('a',), ('l', 'a'), ('h', 'u', 'm', 'a', 'n', 'idad'), ('o',), ('por',), ('i', 'n', 'a', 'c', 'cion'), ('per', 'mi', 't', 'i', 'r'), ('que',), ('l', 'a'), ('h', 'u', 'm', 'a', 'n', 'idad'), ('s', 'u', 'f', 'r', 'a'), ('d', 'a', 'ñ', 'o', 's')]\n",
      "dañar [('d', 'a', 'ñ', 'a', 'r')]\n",
      "inaccion [('i', 'n', 'a', 'c', 'cion')]\n",
      "humanidad [('h', 'u', 'm', 'a', 'n', 'idad')]\n",
      "robot [('r', 'o', 'b', 'o', 't')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer: TokenizationMethod = UnigramLanguageModel()\n",
    "\n",
    "for vocab in unigram_vocabs:\n",
    "    print(f\"Size: {len(vocab)}\")\n",
    "    print(\"Text: \", tokenizer.tokenize_text(vocab,text_to_tokenize))\n",
    "    for word_to_tokenize in concrete_words_to_analyze:\n",
    "        print(word_to_tokenize, tokenizer.tokenize_text(vocab,word_to_tokenize))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size: 250\n",
      "Text:  ['un', ' ', 'r', '##o', '##b', '##o', '##t', ' ', 'n', '##o', ' ', 'p', '##u', '##e', '##d', '##e', ' ', 'd', '##a', '##ñ', '##a', '##r', ' ', 'a', ' ', 'l', '##a', ' ', 'h', '##u', '##m', '##a', '##n', '##i', '##d', '##a', '##d', ' ', 'o', ' ', 'p', '##o', '##r', ' ', 'in', '##a', '##c', '##c', '##i', '##o', '##n', ' ', 'p', '##e', '##r', '##m', '##i', '##t', '##i', '##r', ' ', 'qu', '##e', ' ', 'l', '##a', ' ', 'h', '##u', '##m', '##a', '##n', '##i', '##d', '##a', '##d', ' ', 's', '##u', '##f', '##r', '##a', ' ', 'd', '##a', '##ñ', '##o', '##s']\n",
      "dañar ['d', '##a', '##ñ', '##a', '##r']\n",
      "inaccion ['in', '##a', '##c', '##c', '##i', '##o', '##n']\n",
      "humanidad ['h', '##u', '##m', '##a', '##n', '##i', '##d', '##a', '##d']\n",
      "robot ['r', '##o', '##b', '##o', '##t']\n",
      "\n",
      "Size: 1000\n",
      "Text:  ['un', ' ', 'r', '##o', '##b', '##o', '##t', ' ', 'n', '##o', ' ', 'pu', '##e', '##d', '##e', ' ', 'd', '##a', '##ñ', '##a', '##r', ' ', 'a', ' ', 'l', '##a', ' ', 'h', '##u', '##m', '##a', '##n', '##i', '##d', '##a', '##d', ' ', 'o', ' ', 'p', '##o', '##r', ' ', 'in', '##a', '##c', '##c', '##i', '##o', '##n', ' ', 'p', '##e', '##r', '##m', '##i', '##t', '##i', '##r', ' ', 'qu', '##e', ' ', 'l', '##a', ' ', 'h', '##u', '##m', '##a', '##n', '##i', '##d', '##a', '##d', ' ', 'su', '##f', '##r', '##a', ' ', 'd', '##a', '##ñ', '##o', '##s']\n",
      "dañar ['d', '##a', '##ñ', '##a', '##r']\n",
      "inaccion ['in', '##a', '##c', '##c', '##i', '##o', '##n']\n",
      "humanidad ['h', '##u', '##m', '##a', '##n', '##i', '##d', '##a', '##d']\n",
      "robot ['r', '##o', '##b', '##o', '##t']\n",
      "\n",
      "Size: 4000\n",
      "Text:  ['un', ' ', 'r', '##o', '##b', '##o', '##t', ' ', 'no', ' ', 'pu', '##e', '##d', '##e', ' ', 'd', '##a', '##ñ', '##a', '##r', ' ', 'a', ' ', 'l', '##a', ' ', 'hum', '##a', '##n', '##i', '##d', '##a', '##d', ' ', 'o', ' ', 'p', '##o', '##r', ' ', 'in', '##a', '##c', '##c', '##i', '##o', '##n', ' ', 'p', '##e', '##r', '##m', '##i', '##t', '##i', '##r', ' ', 'qu', '##e', ' ', 'l', '##a', ' ', 'hum', '##a', '##n', '##i', '##d', '##a', '##d', ' ', 'sufr', '##a', ' ', 'd', '##a', '##ñ', '##o', '##s']\n",
      "dañar ['d', '##a', '##ñ', '##a', '##r']\n",
      "inaccion ['in', '##a', '##c', '##c', '##i', '##o', '##n']\n",
      "humanidad ['hum', '##a', '##n', '##i', '##d', '##a', '##d']\n",
      "robot ['r', '##o', '##b', '##o', '##t']\n",
      "\n",
      "Size: 16000\n",
      "Text:  ['un', ' ', 'rob', '##o', '##t', ' ', 'no', ' ', 'puede', ' ', 'dañar', ' ', 'a', ' ', 'l', '##a', ' ', 'hum', '##a', '##n', '##i', '##d', '##a', '##d', ' ', 'o', ' ', 'p', '##o', '##r', ' ', 'in', '##a', '##c', '##ci', '##o', '##n', ' ', 'p', '##e', '##r', '##m', '##i', '##t', '##i', '##r', ' ', 'que', ' ', 'l', '##a', ' ', 'hum', '##a', '##n', '##i', '##d', '##a', '##d', ' ', 'sufra', ' ', 'daño', '##s']\n",
      "dañar ['dañar']\n",
      "inaccion ['in', '##a', '##c', '##ci', '##o', '##n']\n",
      "humanidad ['hum', '##a', '##n', '##i', '##d', '##a', '##d']\n",
      "robot ['rob', '##o', '##t']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer: TokenizationMethod = WordPiece()\n",
    "\n",
    "for vocab in wp_vocabs:\n",
    "    print(f\"Size: {len(vocab)}\")\n",
    "    print(\"Text: \", tokenizer.tokenize_text(vocab,text_to_tokenize))\n",
    "    for word_to_tokenize in concrete_words_to_analyze:\n",
    "        print(word_to_tokenize, tokenizer.tokenize_text(vocab,word_to_tokenize))\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
